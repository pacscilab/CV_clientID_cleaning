---
title: "Supplemenetary materials for Quantifying and Reducing Speaker Heterogeneity within the Common Voice Corpus for Phonetic Analysis (Interspeech 2025)"
author:
  - name: Miao Zhang
    affiliation: University of Zurich
  - name: Aref Farhadipour
    affiliation: University of Zurich
  - name: Annie Baker
    affiliation: University of Zurich
  - name: Jiachen Ma
    affiliation: University of Zurich
  - name: Bogdan Pricop
    affiliation: University of Zurich
  - name: Eleanor Chodroff
    affiliation: University of Zurich
format: html
---



```{r load packages}
#| echo: FALSE
#| message: FALSE
#| warning: FALSE
# Set up the packages and functions needed to perform the analysis.
# List of required packages
required_packages <- c(
  "tidyverse",
  "ggdist",
  "ggview",
  "irr",
  "lme4",
  "ggeffects"
)

# Install any packages that are not already installed
installed_packages <- rownames(installed.packages())
packages_to_install <- setdiff(required_packages, installed_packages)

if (length(packages_to_install) > 0) {
  install.packages(packages_to_install)
}

# Load all required packages
invisible(lapply(required_packages, library, character.only = TRUE))

# Set the plotting theme
theme_set(theme_ggdist())

# The function to create score bins
create_scoreBin <- function(df) {
  df <- df |> 
    mutate(
      score_bin = case_when(
        score < 0.1 ~ "\u003c 0.1",
        score < 0.2 ~ "\u003c 0.2",
        score < 0.3 ~ "\u003c 0.3",
        score < 0.4 ~ "\u003c 0.4",
        score < 0.5 ~ "\u003c 0.5",
        .default = "\u003c 1.0"),
      score_bin = factor(score_bin, levels = c("\u003c 0.1", 
                                               "\u003c 0.2", 
                                               "\u003c 0.3", 
                                               "\u003c 0.4", 
                                               "\u003c 0.5", 
                                               "\u003c 1.0")))
  return(df)
}
```

## Similarity scores

Download the similarity score file from [VoxCommunis](https://huggingface.co/datasets/pacscilab/VoxCommunis/blob/main/similarity_scores.txt).

```{r get the similarity scores}
col_names <- c("test", "enroll", "score")
# Replace the file path with the one on your computer
scores <- read_delim("similarity_scores.txt", col_names = col_names, show_col_types = FALSE)
threshold = 0.354
scores <- scores |> mutate(lang_code = str_extract(enroll, "(?<=voice_)[^_]+"), # Get the languages
                           under_threshold = if_else(score < threshold, T, F))
glimpse(scores)

summary(scores$score)
scores_by_lang <- scores |>
  group_by(lang_code) |> 
  summarise(q1 = quantile(score, 0.25), 
            median = median(score), 
            q3 = quantile(score, 0.75))
summary(scores_by_lang$q1)
summary(scores_by_lang$q3)
```

Plot the distribution of the similarity scores across all languages.

```{r histogram of scores}
scores |> 
  ggplot(aes(score)) +
  geom_histogram(aes(fill = after_stat(density)), bins = 40,
                 show.legend = F) +
  #geom_vline(xintercept = threshold, color = "red3", linewidth = 0.5) +
  scale_fill_viridis_c() +
  scale_y_continuous(expand = expansion(mult = c(0, .1)),
                     label = scales::label_number(scale = 1)) +
  scale_x_continuous(breaks = c(0, 0.4, 0.8)) +
  labs(y = expression("Count"), x = "Similarity score") +
  coord_cartesian(xlim = c(-0.1, 1.0)) +
  theme(panel.grid.major.x = element_line(linewidth = 0.4),
        panel.grid.minor.x = element_line(linewidth = 0.2),
        panel.grid.major.y = element_line(linewidth = 0.4),
        axis.text.x = element_text(size=16),
        axis.text.y = element_text(size=16),
        axis.title.x = element_text(size=16),
        axis.title.y = element_text(size=16)) 
ggsave("~/Desktop/hist_score.pdf", plot=last_plot(), height = 3, width = 7, dpi = 300)
```


The next step require downloading speaker files from [VoxCommunis Corpus](https://huggingface.co/datasets/pacscilab/VoxCommunis).

```{r get speaker files}
# Replace the path below with the one on your own computer
spkr_dir <- "~/VoxCommunis_Huggingface/speaker_files"
spkr_files <- list.files(spkr_dir, pattern = "*.tsv", full.names = T)

spkr_files <- map_dfr(spkr_files, \(x) read_tsv(x, col_select = c("path", "speaker_id"), show_col_types = FALSE))

spkr_files <- mutate(spkr_files, speaker_id = paste(str_extract(path, "(?<=voice_)[^_]+"), speaker_id, sep = "_"))
spkr_files <- rename(spkr_files, test = path)
spkr_files <- mutate(spkr_files, test = str_replace(test, ".mp3", ""))

scores <- mutate(scores, test = str_replace(test, ".wav", ""))

scores <- inner_join(scores, spkr_files, by = "test")
glimpse(scores)

prop_under_threshold_by_lang <- scores |> 
  summarize(prop_threshold = sum(under_threshold)/n(), .by = lang_code)
```

Evaluate how much the client IDs are affected by the heterogeneous speaker issue by looking into how many client IDs that contain recordings from multiple speakers there are.

```{r get data ready for evaluating speaker heterogeneity}
speaker_id_impact <- scores |> 
  summarize(n = n() + 1, prop_threshold = sum(under_threshold)/(n() + 1), .by = speaker_id) |> 
  mutate(affected = if_else(prop_threshold > 0, T, F),
         more_than_10 = if_else(prop_threshold > 0.10, T, F),
         lang = str_extract(speaker_id, "^[^_]+")) 

n_affected_more_than_10 <- speaker_id_impact |> 
  summarize(n_affected = sum(more_than_10),
            n_total = n(), 
            perc = n_affected/n_total,
            .by = lang) 

prop_more_than_100 <- scores |> summarize(n = n() + 1, .by = c(lang_code, speaker_id)) |> 
  mutate(more_than_100 = if_else(n >= 100, T, F)) |> 
  summarize(prop_more_than_100 = sum(more_than_100)/n(),
            n = n(),
            .by = lang_code)

summary(prop_under_threshold_by_lang$prop_threshold)
nrow(subset(prop_under_threshold_by_lang, prop_threshold < 0.1))
nrow(subset(speaker_id_impact, prop_threshold < 0.1))
nrow(subset(speaker_id_impact, prop_threshold < 0.1)) / nrow(speaker_id_impact)
affected_ids <- speaker_id_impact |>
  group_by(lang, more_than_10) |>
  summarise(greaterThan10 = n()) |>
  pivot_wider(names_from = more_than_10, values_from = greaterThan10) |>
  rename("good" = `FALSE`,
         "bad" = `TRUE`) |>
  mutate(bad = ifelse(is.na(bad), 0, bad), 
         total = sum(good + bad),
         prop = bad/total)
summary(affected_ids$prop)

```

Plot the the degree to which client IDs and languages are affected.

```{r plot the heterogeneity results}
# The proportion of files in each language with a score under the threshold
prop_under_threshold_by_lang |> 
  ggplot(aes(reorder(lang_code, -prop_threshold), prop_threshold, fill = prop_threshold)) +
  geom_col() +
  scale_fill_viridis_c() +
  scale_y_continuous(lim = c(0,0.25), 
                     expand = expansion(mult = c(0, .01))) +
  guides(fill = "none") +
  #ylab("Prop. utterances") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.35, size = 8),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        #axis.title.y = element_text(size = 16),
        axis.text.y = element_text(size = 16),
        axis.ticks.x = element_blank(),
        panel.grid.major.y = element_line(linetype = 3),
        legend.title = element_blank())
ggsave("~/Desktop/dist_scores.pdf", plot=last_plot(), dpi = 300, height = 3, width = 7)

# The proportion of client IDs that have more than 10% of the associated recordings with a score lower than the threshold
ggplot(n_affected_more_than_10, aes(reorder(lang, desc(perc)), perc, fill = perc)) +
  geom_col() +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  scale_fill_viridis_c() +
  guides(fill = "none") +
  #ylab("Prop. client IDs") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.35, size = 8),
        axis.title.x = element_blank(),
        axis.title.y = element_blank(),
        #axis.title.y = element_text(size = 16),
        axis.text.y = element_text(size = 16),
        axis.ticks.x = element_blank(),
        legend.title = element_blank(),
        panel.grid.major.y = element_line(linetype = 3))
ggsave("~/Desktop/under_threshold_perc.pdf", plot=last_plot(), dpi = 300, height = 3, width = 7)
```


## Auditing result: round 1

Read in the auditing result from round 1.

```{r get audit r1 data}
audit_r1 <- read_csv("audit_r1.csv") |> 
  create_scoreBin() |> 
  mutate(validation = factor(validation, 
                             levels = c("Different Speaker", "Audio Quality Issue", 
                                        "Missing Speech", "Not Sure", "Same Speaker")))
glimpse(audit_r1)
```

Plot the round 1 results.

```{r r1 results plotting}
audit_r1$validation <- factor(audit_r1$validation, levels = c("Different Speaker", "Same Speaker", "Audio Quality Issue", "Missing Speech", "Not Sure"))
ggplot(audit_r1, aes(x = score_bin, fill = validation)) + 
  stat_count(position = "dodge") +
  scale_fill_manual(values = c("#5a2b75", "#3d9e96", "#f3e740", "#4a6b99", "#6bcd72")) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  #theme_ggdist(base_size = 6.5) +
  ylab("Count") + 
  guides(fill=guide_legend(nrow=2,byrow=TRUE)) +
  theme(axis.title.x = element_blank(),
        panel.grid.major.y = element_line(linetype = 3, linewidth = 0.8),
        axis.text = element_text(size = 16),
        axis.title.y = element_text(size = 16),
        legend.text = element_text(size = 15),
        legend.position = "bottom",
        legend.title = element_blank()) 
ggsave("~/Desktop/vxc-spkr-val.pdf", plot=last_plot(), dpi = 300, height = 3, width = 7)
```

## Auditing result: round 1

Get the round 2 data.

```{r get audit r2 data}
audit_r2 <- read_csv("audit_r2.csv") |> create_scoreBin()
glimpse(audit_r2)
```

Calculate the Fleiss' Kappa.

```{r fleiss kappa}
kappa_dat <- audit_r2[, colnames(audit_r2) %in% c("validation1", "validation2", "validation3", "validation4", "validation5")]
kappam.fleiss(kappa_dat)
```

Fit a GLMER model to evaluate the threshold to reject same speaker hypothesis.

```{r make the data into long format}
audit_r2_long <- audit_r2 |> 
  pivot_longer(cols = starts_with("person"), names_to = ("person"), values_to = "name") |>
  pivot_longer(cols = starts_with("validation"), names_to = ("validation"), values_to = "val") |>
  filter(name == "Annotator1" & validation == "validation1" |
           name == "Annotator2" & validation == "validation2" |
           name == "Annotator3" & validation == "validation3" |
           name == "Annotator4" & validation == "validation4" |
           name == "Annotator5" & validation == "validation5")
glimpse(audit_r2_long)
```

```{r fit the glmer model}
samediff_all <- audit_r2_long |> 
  filter(val %in% c("Same Speaker","Different Speaker")) |>
  mutate(nVal = ifelse(val == "Same Speaker", 1, 0))
mod <- glmer(nVal ~ score + (1 | person) + (0 + score | person) + (1 | lang) + (0 + score | lang), family = "binomial", data = samediff_all)
summary(mod)
```

Get the threshold.

```{r obtain the threshold}
threshold <- -fixef(mod)[1]/fixef(mod)[2]
threshold
```

Plot the model result.

```{r plot the threshold}
samediff_all$predicted <- predict(mod, type = "response", re.form = NA)
samediff_all$predicted_person <- predict(mod, type = "response")
preds <- ggpredict(mod, terms = "score[all]", interval = "confidence")

ggplot(samediff_all, aes(x = score, y = nVal)) +
  geom_jitter(alpha = 0.08, width = 0.05, height = 0.05) +
  # Confidence interval ribbon
  geom_ribbon(data = preds, aes(x = x, ymin = conf.low, ymax = conf.high), 
              fill = "deepskyblue3", alpha = 0.2, inherit.aes = FALSE) +
  # Model prediction line
  geom_line(data = preds, aes(x = x, y = predicted), 
            color = "deepskyblue3", size = 1.2, inherit.aes = FALSE) +
  scale_x_continuous(limits = c(-0.2, 1)) +
  labs(x = "Similarity score", y = "Probability") +
  geom_vline(xintercept = threshold, linetype = "dashed", color = "red") +
  theme(panel.grid.minor = element_blank(),
        axis.text.y = element_text(size = 16),
        axis.title.y = element_text(size = 16),
        axis.text.x = element_text(size = 16),
        axis.title.x = element_text(size = 16)) 
ggsave("~/Desktop/turning_point.pdf", plot=last_plot(), dpi = 300, height = 3, width = 7)

```


